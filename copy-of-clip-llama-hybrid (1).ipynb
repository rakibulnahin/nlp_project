{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/rakibulnahin/nlp_project.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZ1NgO5a0shj","outputId":"2dad7420-0b21-4247-a224-174ffbf99264","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:22:05.374500Z","iopub.execute_input":"2025-06-17T01:22:05.375088Z","iopub.status.idle":"2025-06-17T01:22:06.469976Z","shell.execute_reply.started":"2025-06-17T01:22:05.375064Z","shell.execute_reply":"2025-06-17T01:22:06.469020Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'nlp_project'...\nremote: Enumerating objects: 82, done.\u001b[K\nremote: Counting objects: 100% (82/82), done.\u001b[K\nremote: Compressing objects: 100% (77/77), done.\u001b[K\nremote: Total 82 (delta 6), reused 80 (delta 4), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (82/82), 6.11 MiB | 28.32 MiB/s, done.\nResolving deltas: 100% (6/6), done.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:16:05.491231Z","iopub.execute_input":"2025-06-17T01:16:05.491523Z","iopub.status.idle":"2025-06-17T01:16:05.615090Z","shell.execute_reply.started":"2025-06-17T01:16:05.491499Z","shell.execute_reply":"2025-06-17T01:16:05.614142Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%capture\n!pip install -U transformers accelerate peft bitsandbytes datasets\n","metadata":{"id":"O3ZWm4ylH5gi","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:16:05.616205Z","iopub.execute_input":"2025-06-17T01:16:05.616530Z","iopub.status.idle":"2025-06-17T01:17:34.740961Z","shell.execute_reply.started":"2025-06-17T01:16:05.616498Z","shell.execute_reply":"2025-06-17T01:17:34.740124Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import huggingface_hub\ntoken = \"token\"\nhuggingface_hub.login(token)","metadata":{"id":"q0uGHbWTIoqY","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:17:34.743137Z","iopub.execute_input":"2025-06-17T01:17:34.743768Z","iopub.status.idle":"2025-06-17T01:17:35.308476Z","shell.execute_reply.started":"2025-06-17T01:17:34.743742Z","shell.execute_reply":"2025-06-17T01:17:35.307901Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom transformers import (\n    CLIPVisionModel, CLIPImageProcessor,\n    LlamaForCausalLM, LlamaTokenizer,\n    AutoProcessor, TrainingArguments,\n    Trainer, AutoTokenizer\n)\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom PIL import Image\nimport pandas as pd\nfrom transformers.utils import logging\nimport os\nlogging.set_verbosity_error()  # Suppress too much logging\nfrom tqdm import tqdm\n","metadata":{"id":"EfDmo_1nJAON","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:17:35.309126Z","iopub.execute_input":"2025-06-17T01:17:35.309378Z","iopub.status.idle":"2025-06-17T01:18:00.920873Z","shell.execute_reply.started":"2025-06-17T01:17:35.309343Z","shell.execute_reply":"2025-06-17T01:18:00.920097Z"}},"outputs":[{"name":"stderr","text":"2025-06-17 01:17:46.336117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750123066.547514      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750123066.609128      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"# ----- Augmentaion------\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),       # Randomly crop and resize to 224x224\n    transforms.RandomHorizontalFlip(),      # Randomly flip the image horizontally\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Randomly change brightness, etc.\n    # Add more transforms as needed, e.g., transforms.RandomRotation, transforms.GaussianBlur\n    transforms.RandomGrayscale(p=0.5),\n    transforms.ToTensor(), # Convert PIL Image to Tensor (C, H, W)\n    # The image_processor will handle the final normalization based on its model's requirements\n    # If your image_processor *doesn't* handle normalization, you'd add it here:\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(256),             # Resize the smaller edge to 256\n    transforms.CenterCrop(224),         # Crop the center to 224x224\n    transforms.ToTensor(),              # Convert PIL Image to Tensor\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # If not handled by processor\n])\n\n# ---- CONFIG ----\nimage_encoder_name = \"openai/clip-vit-base-patch32\"\nllm_name = \"meta-llama/Llama-2-7b-chat-hf\"\nconfident_pth = \"/kaggle/working/nlp_project/my_dataset/confident_feedback.csv\"\nscared_pth = \"/kaggle/working/nlp_project/my_dataset/scared_feedback.csv\"\nimage_root = \"/kaggle/working/nlp_project/my_dataset\"\nmax_length = 128\nbatch_size = 4\nnum_epochs = 10\nlr = 2e-5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"IpJWDCJPPaWl","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:37:54.933706Z","iopub.execute_input":"2025-06-17T01:37:54.933984Z","iopub.status.idle":"2025-06-17T01:37:54.940003Z","shell.execute_reply.started":"2025-06-17T01:37:54.933964Z","shell.execute_reply":"2025-06-17T01:37:54.939340Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Vision Encoder","metadata":{}},{"cell_type":"code","source":"# ---- Load Vision Encoder ----\nvision_encoder = CLIPVisionModel.from_pretrained(image_encoder_name).to(device)\nvision_encoder.eval().requires_grad_(False)\nvision_processor = CLIPImageProcessor.from_pretrained(image_encoder_name)","metadata":{"id":"rEGbnINlk2AW","colab":{"base_uri":"https://localhost:8080/","height":877},"outputId":"e4d9e5c4-27d7-4d86-8fd9-2c1b86738241","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:18:00.929270Z","iopub.execute_input":"2025-06-17T01:18:00.929561Z","iopub.status.idle":"2025-06-17T01:18:05.393832Z","shell.execute_reply.started":"2025-06-17T01:18:00.929536Z","shell.execute_reply":"2025-06-17T01:18:05.392812Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"212214891bb34f17b6a7a51b2cee27d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601f65de77704fcb98e0cf2690e1c3e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c1b46276ce4be08227f00716031eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61ba77fb9ff4d25af805a7f554e637b"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Llama Tokenizer + Model","metadata":{}},{"cell_type":"code","source":"# ---- Load Tokenizer ----\ntokenizer = AutoTokenizer.from_pretrained(llm_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# ---- Load LLaMA Language Model with LoRA + 4-bit ----\nlanguage_model = LlamaForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    device_map=\"auto\",\n    load_in_4bit=True\n)\nlanguage_model.gradient_checkpointing_enable()\nlanguage_model = prepare_model_for_kbit_training(language_model)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nlanguage_model = get_peft_model(language_model, lora_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:18:05.394969Z","iopub.execute_input":"2025-06-17T01:18:05.395256Z","iopub.status.idle":"2025-06-17T01:20:36.732558Z","shell.execute_reply.started":"2025-06-17T01:18:05.395229Z","shell.execute_reply":"2025-06-17T01:20:36.731994Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfed977c5d6c4a9388cc5842d709c1f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629885b6658544e8b9502f7d8813d6aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fedc3ba66a824d41807262b55af54fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6b88f3d0e2440bb1c32091231ce1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c918f648594728b5f5e57b530b5ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0e1cde97b248a4ac7068abe17a7e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc61184318b41f8bd6ac3a59ac78401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8be4113ff68d4dcba794e99185816003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2101a5088c414bba576aeddd43c517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7764d0bf5e40679b42feae95c70203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d33437e436524f61b140a43acf21741f"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# ---- Dataset ----\nclass ImageTextDataset(Dataset):\n    def __init__(self, csv_path, emotion, image_processor, tokenizer, max_length=128, transformer=None):\n        self.data = pd.read_csv(csv_path)\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.emotion = emotion\n        self.transformer = transformer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        image_path = os.path.join(image_root, row['image'])\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transformer:\n            image = self.transformer(image)\n\n        image_tensor = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        full_label = f'you look {self.emotion}. {row[\"feedback\"]}'\n        label_encoding = self.tokenizer(full_label, padding='max_length', truncation=True,\n                                        max_length=self.max_length, return_tensors='pt')\n\n        return {\n            \"pixel_values\": image_tensor,\n            \"input_ids\": label_encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": label_encoding[\"attention_mask\"].squeeze(0)\n        }\n\ndef custom_collate(features):\n    return {\n        \"pixel_values\": torch.stack([f[\"pixel_values\"] for f in features]),\n        \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features])\n    }\n\n# ---- Load Data ----\nscared_dataset = ImageTextDataset(scared_pth, \"scared\", vision_processor, tokenizer, max_length, train_transform)\nconfident_dataset = ImageTextDataset(confident_pth, \"confident\", vision_processor, tokenizer, max_length, train_transform)\ndataset = ConcatDataset([scared_dataset, confident_dataset, scared_dataset, confident_dataset, scared_dataset, confident_dataset])\nprint(len(dataset))\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:38:07.190764Z","iopub.execute_input":"2025-06-17T01:38:07.191446Z","iopub.status.idle":"2025-06-17T01:38:07.208706Z","shell.execute_reply.started":"2025-06-17T01:38:07.191412Z","shell.execute_reply":"2025-06-17T01:38:07.207904Z"}},"outputs":[{"name":"stdout","text":"165\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\n\n# ---- Projector ----\nprojector = nn.Sequential(\n    nn.Linear(vision_encoder.config.hidden_size, language_model.config.hidden_size),\n    nn.Tanh()\n).to(device)\n\noptimizer = torch.optim.AdamW(list(language_model.parameters()) + list(projector.parameters()), lr=lr)\n\n# ---- Training Loop ----\nlanguage_model.train()\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for batch in pbar:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        with torch.no_grad():\n            vision_outputs = vision_encoder(pixel_values=pixel_values)\n            image_embeds = vision_outputs.last_hidden_state.mean(dim=1)\n\n        prefix_embeds = projector(image_embeds).unsqueeze(1)\n        text_embeds = language_model.model.model.embed_tokens(input_ids)\n        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)\n\n        # Adjust attention and labels\n        prefix_attention = torch.ones((input_ids.size(0), 1), device=device)\n        combined_attention_mask = torch.cat([prefix_attention, attention_mask], dim=1)\n\n        labels = input_ids.clone()\n        labels = torch.cat([torch.full((labels.size(0), 1), -100, dtype=torch.long, device=device), labels[:, :-1]], dim=1)\n\n        # Truncate to max length\n        inputs_embeds = inputs_embeds[:, :max_length, :]\n        combined_attention_mask = combined_attention_mask[:, :max_length]\n        labels = labels[:, :max_length]\n\n        outputs = language_model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=combined_attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        running_loss += loss.item()\n        pbar.set_postfix(loss=loss.item())\n\n    print(f\"‚úÖ Epoch {epoch+1} complete | Avg Loss: {running_loss / len(train_loader):.4f}\")\n","metadata":{"id":"OPj7_538k19y","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:38:51.146259Z","iopub.execute_input":"2025-06-17T01:38:51.146615Z","iopub.status.idle":"2025-06-17T02:20:52.202015Z","shell.execute_reply.started":"2025-06-17T01:38:51.146595Z","shell.execute_reply":"2025-06-17T02:20:52.201205Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10:   0%|          | 0/42 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nEpoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:05<00:00,  5.86s/it, loss=1.62] \n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 1 complete | Avg Loss: 5.3453\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:10<00:00,  5.96s/it, loss=0.664]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 2 complete | Avg Loss: 0.8710\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:12<00:00,  6.01s/it, loss=0.665]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 3 complete | Avg Loss: 0.7818\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:16<00:00,  6.10s/it, loss=0.675]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 4 complete | Avg Loss: 0.6949\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:16<00:00,  6.10s/it, loss=0.62] \n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 5 complete | Avg Loss: 0.5974\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:14<00:00,  6.06s/it, loss=0.416]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 6 complete | Avg Loss: 0.4999\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:11<00:00,  5.99s/it, loss=0.331]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 7 complete | Avg Loss: 0.4322\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:11<00:00,  5.98s/it, loss=0.333]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 8 complete | Avg Loss: 0.3817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:11<00:00,  5.98s/it, loss=0.369]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 9 complete | Avg Loss: 0.3319\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [04:11<00:00,  5.98s/it, loss=0.306]","output_type":"stream"},{"name":"stdout","text":"‚úÖ Epoch 10 complete | Avg Loss: 0.2804\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Generate Feedback","metadata":{}},{"cell_type":"code","source":"def generate_feedback(image_path, max_new_tokens=30):\n    image = Image.open(image_path).convert(\"RGB\")\n    image = val_transform(image)\n    pixel_value = vision_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0).to(device)\n    vision_encoder.eval()\n    language_model.eval()\n\n    with torch.no_grad():\n        vision_feat = vision_encoder(pixel_value.unsqueeze(0)).last_hidden_state.mean(dim=1)\n        prefix_embed = projector(vision_feat).unsqueeze(1)\n\n        # --- FIX STARTS HERE ---\n        # Prime the model with the expected starting phrase, e.g., \"you look\"\n        # The model will then try to complete this based on the image embedding\n        # You might need to experiment with what exact phrase works best.\n        # \"you look\" implies it should follow with an emotion.\n        # If you want it to directly start with feedback, you'd need to train it that way.\n        prompt_text = \"you look\" # Or \"The person looks\" depending on your desired output\n        input_ids = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n        # Add special tokens=False to avoid adding <s> again if you're concatenating.\n        # If you want <s> at the very beginning of the whole sequence, ensure your training includes it,\n        # otherwise, keep add_special_tokens=True for the first tokenized part.\n        # For simplicity, let's assume we want \"you look\" to be the very start of the text part.\n\n        token_embed = language_model.model.model.embed_tokens(input_ids)\n        input_embed = torch.cat([prefix_embed, token_embed], dim=1)\n\n        output_ids = language_model.generate(\n            inputs_embeds=input_embed,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_p=0.9,\n            temperature=1.0,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    # Decode the output, starting after the initial prompt you provided.\n    # The output_ids will include the tokens for \"you look\" that you fed in.\n    # So, you need to skip those when decoding or use a slice if you generated from the very beginning.\n    # A simpler way is to just decode everything and then clean it up.\n    # For now, let's decode the whole thing and strip the prompt, if it's included.\n    decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # If the generation includes the prompt text, you might want to remove it from the beginning\n    # For example, if output is \"you look confident. Keep going!\", and your prompt was \"you look\",\n    # you might want to ensure the prompt part isn't duplicated if the model generates it.\n    # However, for now, the model will likely continue from where \"you look\" leaves off.\n    return decoded_output\n\n\n# --- Example Usage ---\ntest_image = \"/kaggle/working/nlp_project/my_dataset/scared/21.png\"  # Replace with your image path\nresult = generate_feedback(test_image)\nprint(\"\\nüß† Generated Feedback:\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:24:59.838709Z","iopub.execute_input":"2025-06-17T02:24:59.839005Z","iopub.status.idle":"2025-06-17T02:25:01.576199Z","shell.execute_reply.started":"2025-06-17T02:24:59.838984Z","shell.execute_reply":"2025-06-17T02:25:01.575307Z"}},"outputs":[{"name":"stdout","text":"\nüß† Generated Feedback:\nsurely. you look scared. let us talk about something else.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:26:57.435728Z","iopub.execute_input":"2025-06-17T02:26:57.435994Z","iopub.status.idle":"2025-06-17T02:26:57.439675Z","shell.execute_reply.started":"2025-06-17T02:26:57.435976Z","shell.execute_reply":"2025-06-17T02:26:57.438908Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"path = \"/kaggle/working/nlp_project/my_dataset/confident\"\nimages = os.listdir(path)\nresponses = []\nfor image in images:\n    result = generate_feedback(path+\"/\"+image)\n    responses.append(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:31:26.900823Z","iopub.execute_input":"2025-06-17T02:31:26.901661Z","iopub.status.idle":"2025-06-17T02:31:59.874638Z","shell.execute_reply.started":"2025-06-17T02:31:26.901635Z","shell.execute_reply":"2025-06-17T02:31:59.874088Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"responses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:32:07.812409Z","iopub.execute_input":"2025-06-17T02:32:07.813188Z","iopub.status.idle":"2025-06-17T02:32:07.818321Z","shell.execute_reply.started":"2025-06-17T02:32:07.813165Z","shell.execute_reply":"2025-06-17T02:32:07.817607Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['hopefully. you look strong.',\n '‚úø',\n 'You look concerned.–Æ–Ω—ã–π –¥–æ–±—Ä—ã, –≥–æ—Ä–∞–∑–¥–∞..',\n '–í–∏–∫–∏–ø–µ–¥–∏–∏',\n 'you look scared, be brave.–â',\n \"lets do something that scares you, but doesn't make you any less confident.\",\n \"everyone looks worried right now., you are doing the best you can, let's trust in the process you're trying your best\",\n 'you look scared.and worried.now take a deep breath.',\n 'you look scared.‚Äô you do not need to be scared.',\n 'nobody is perfect. you are doing a great job.',\n 'nobody is perfect. you are doing great.',\n 'gaben',\n 'nobody is perfect. a good start counts, too.',\n '–µ–≥–æ. you look great.',\n 'nobody is perfect. Your contribution is valuable.',\n \"–¥—Ä–∂–∞, relax, don't be nervous, just be yourself, confidence is being sure of yourself\",\n '√§tz, a balanced heart, you are not alone.–â',\n 'you look scared.',\n '–¥—Ä–∂–∞–≤–µ',\n \"you look worry if you can't relax, you don\",\n \"hopefully together.you look scared.let's keep going.\",\n 'lets how do you feel?',\n 'gaben',\n 'you look scared. Your accomplishments matter. You are doing a great job.',\n 'You look scared. You look scared. You‚Äôre doing great.',\n 'lets focus. your faith counts.']"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"path = \"/kaggle/working/nlp_project/my_dataset/scared\"\nimages = os.listdir(path)\nresponses = []\nfor image in images:\n    print(image)\n    result = generate_feedback(path+\"/\"+image)\n    responses.append(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:33:07.598909Z","iopub.execute_input":"2025-06-17T02:33:07.599510Z","iopub.status.idle":"2025-06-17T02:33:49.280404Z","shell.execute_reply.started":"2025-06-17T02:33:07.599479Z","shell.execute_reply":"2025-06-17T02:33:49.279861Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"7.png\n5.png\n14.png\n9.png\n13.png\n6.png\n28.png\n17.png\n2.png\n1.png\n4.png\n26.png\n20.png\n3.png\n15.png\n24.png\n18.png\n10.png\n21.png\n29.png\n22.png\n16.png\n8.png\n25.png\n19.png\n27.png\n23.png\n12.png\n11.png\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"responses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:39:00.614999Z","iopub.execute_input":"2025-06-17T02:39:00.615620Z","iopub.status.idle":"2025-06-17T02:39:00.620244Z","shell.execute_reply.started":"2025-06-17T02:39:00.615598Z","shell.execute_reply":"2025-06-17T02:39:00.619692Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"[\"you look scared. You look worried. It's okay to take a break. Take a moment to gather yourself.\",\n 'lets you in.injustice does not define you.when did you last laugh?',\n 'hopefully your own efforts are valuable.–©–µ –≥–ª–æ–±–∞–ª –∑–∞ —Å—Ç–µ–ª–æ–º.',\n '–µ–≥–æ. You look great.',\n '–±—Ä–æ—ò–∞',\n '–æ–∫—Ä—É–≥—É, just relax.com.',\n 'everybody looks tired.it is okay to feel the way you feel',\n 'nobody pretends perfection',\n 'nobody is perfect. you are doing the best you can.',\n '√§tz\\nJune 27, 2021 ¬∑ 10:44 AM',\n '—Å–∞–≤–µ–∑',\n '√§tz lang double quote',\n 'You look scared.Learn more about yourself and your goals with a psychology visit.',\n 'you look scared.you look concerned.you look worried.you look unsure.',\n 'You look scared.’£  You look worried. Try to focus on your breath for a few moments and let go of the t',\n \"You look scared. Cynical or sarcastic answers aren't the right ones. Keep it real.\",\n '√§tz lang double quarter long',\n '—Å–∞–≤–µ–∑',\n 'nobody is perfect. You are doing a great job.',\n 'hopefully, trust yourself',\n 'you look scared. You do not have to be perfect to start. Just be present.',\n 'everybody is afraid, just keep on trying your best',\n 'everybody looks scared.',\n 'you look scared.‚Äô just let it go. you don‚Äôt have to get it perfect.',\n 'you look scared.a sense of achievement.',\n 'nobody is perfect. the day is yours to do the best you can.',\n '–µ–≥–æ. you look great.',\n 'you look scared. in your head, it doesn‚Äôt show. you‚Äôre doing the best you can, keep going',\n 'nobody is perfect, just keep going']"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Save the LoRA-adapted LLaMA model\nlanguage_model.save_pretrained(\"/content/saved_model/vlm_lora_llama2\")\n\n# Save the projector layer separately\ntorch.save(projector.state_dict(), \"/content/vlm_projector.pt\")\n","metadata":{"id":"BI9rtz7ek4Wo","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:20:36.786609Z","iopub.status.idle":"2025-06-17T01:20:36.786917Z","shell.execute_reply.started":"2025-06-17T01:20:36.786775Z","shell.execute_reply":"2025-06-17T01:20:36.786793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\n# Load LLaMA base in 4-bit and apply LoRA weights\nbase_model = LlamaForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    load_in_4bit=True,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(base_model, \"/content/vlm_lora_llama2\").eval()\n\n# Load projector\nprojector = nn.Sequential(\n    nn.Linear(vision_encoder.config.hidden_size, model.config.hidden_size),\n    nn.Tanh()\n).to(device)\nprojector.load_state_dict(torch.load(\"/content/vlm_projector.pt\", map_location=device))\n","metadata":{"id":"ic0vk2Q1l-qI","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:20:36.789064Z","iopub.status.idle":"2025-06-17T01:20:36.789668Z","shell.execute_reply.started":"2025-06-17T01:20:36.789536Z","shell.execute_reply":"2025-06-17T01:20:36.789552Z"}},"outputs":[],"execution_count":null}]}